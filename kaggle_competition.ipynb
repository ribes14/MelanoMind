{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2025 Kaggle Competition of AI Applied to Medicine at UC3M\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Welcome to the **2025 Kaggle competition of AI applied to medicine at UC3M**. This project is set up as an **internal Kaggle competition** in which all students will participate. Our real-world challenge for this course will revolve around the **ISIC 2024** dataset, a large collection of skin images used for research in dermatology.\n",
    "\n",
    "\n",
    "Welcome to our simple **ResNet50-based** starter notebook. Below we:\n",
    "1. **Define** a function to load images from HDF5 files.\n",
    "2. **Load** and display our training metadata (no preprocessing).\n",
    "3. **Load** a pretrained **ResNet50** model (we won't fine-tune it).\n",
    "4. **Evaluate** test samples (in a trivial way for demonstration).\n",
    "5. **Generate** a simple `submission.csv` file with the required format.\n",
    "\n",
    "> **Note**: This is a minimal example to help you set up your environment. It doesn’t include any real training or meaningful model inference. Feel free to modify it to perform actual classification (e.g., add custom layers, train on your dataset, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ISIC 2024 Competition Overview\n",
    "\n",
    "The **International Skin Imaging Collaboration (ISIC)** has launched this competition to advance automated skin cancer detection by:\n",
    "\n",
    "- **Improving accuracy** in distinguishing malignant from benign lesions\n",
    "- **Enhancing efficiency** in clinical workflows\n",
    "- **Developing algorithms** that prioritize high-risk lesions\n",
    "- **Reducing mortality rates** by enabling earlier detection\n",
    "\n",
    "### Primary Task\n",
    "\n",
    "You need to **classify skin lesions** as **benign** or **malignant**. For each lesion image (identified by `isic_id`), predict a **probability** in the range [0, 1] indicating the chance that the lesion is malignant.\n",
    "\n",
    "### High-Level Data Summary\n",
    "\n",
    "- The dataset is called **SLICE-3D**, containing **skin lesion images** (JPEG files) cropped from 3D Total Body Photography (TBP).\n",
    "- Each image has metadata in a corresponding `.csv` file, including:\n",
    "  - **Binary diagnostic label** (`target` = 0 or 1)\n",
    "  - **Patient data** (e.g., `age_approx`, `sex`, `anatom_site_general`)\n",
    "  - **Additional attributes** (image source, diagnosis type)\n",
    "\n",
    "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F4972760%2F349a3ae1149d15dc5642063a2d742c88%2Fimage%20type_noexif_240425.jpg?generation=1714060307710359&alt=media)\n",
    "\n",
    "This challenge dataset mimics **non-dermoscopic images** using standardized 15x15 mm “tiles” of lesions from a 3D TBP system. Thousands of patients from multiple continents are represented, creating a broad, diverse dataset.\n",
    "\n",
    "### Task Description & Clinical Context\n",
    "\n",
    "- **Why it matters**: Skin cancer can be deadly if not detected early. Many people lack access to dermatologic care, so accurate AI systems for image-based triage can improve outcomes.\n",
    "- **Key goal**: Develop a binary classifier that identifies malignant lesions from a set of smartphone-quality images.\n",
    "- **Impact**: This technology could help prioritize suspicious lesions (top K) for clinical review, especially in low-resource settings, potentially **saving lives** through earlier detection.\n",
    "\n",
    "### Importance of 3D TBP\n",
    "\n",
    "The **3D Total Body Photography (TBP)** approach captures the entire skin surface in macro resolution. Each lesion on the patient’s body is automatically cropped as a 15x15 mm image tile. These images more closely resemble photos taken by a regular smartphone camera, as opposed to specialized dermoscopy devices.\n",
    "\n",
    "![](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F4972760%2F169b1f691322233e7b31aabaf6716ff3%2Fex-tiles.png?generation=1717700538524806&alt=media)\n",
    "\n",
    "### Clinical Background\n",
    "\n",
    "1. **Major skin cancer types**: Basal Cell Carcinoma (BCC), Squamous Cell Carcinoma (SCC), and Melanoma (most lethal).\n",
    "2. **Early detection** is crucial: Minor surgery can cure many skin cancers if caught in time.\n",
    "3. **Telemedicine implications**: With the rise in remote healthcare, patients often submit low-quality images captured at home. Robust AI models are needed to handle this variability.\n",
    "\n",
    "### Summary\n",
    "\n",
    "- You will build a model to **classify skin lesions** (benign vs. malignant) with probabilities.\n",
    "- The dataset includes **every lesion** from thousands of patients, reflecting real-world diversity.\n",
    "- **3D TBP** and the “ugly duckling sign” concept illustrate the importance of comparing each lesion against the patient’s total lesion landscape.\n",
    "- **Your work** can help improve early detection, prioritizing high-risk cases for clinical evaluation and potentially saving lives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METADATA_COL2DESC = {\n",
    "    \"isic_id\": \"Unique identifier for each image case.\",\n",
    "    \"target\": \"Binary class label (0 = benign, 1 = malignant).\",\n",
    "    \"patient_id\": \"Unique identifier for each patient.\",\n",
    "    \"age_approx\": \"Approximate age of the patient at time of imaging.\",\n",
    "    \"sex\": \"Sex of the patient (male or female).\",\n",
    "    \"anatom_site_general\": \"General location of the lesion on the patient's body.\",\n",
    "    \"clin_size_long_diam_mm\": \"Maximum diameter of the lesion (mm).\",\n",
    "    \"image_type\": \"Type of image captured, as defined in the ISIC Archive.\",\n",
    "    \"tbp_tile_type\": \"Lighting modality of the 3D Total Body Photography (TBP) source image.\",\n",
    "    \"tbp_lv_A\": \"Color channel A (green-red axis in LAB space) inside the lesion.\",\n",
    "    \"tbp_lv_Aext\": \"Color channel A outside the lesion.\",\n",
    "    \"tbp_lv_B\": \"Color channel B (blue-yellow axis in LAB space) inside the lesion.\",\n",
    "    \"tbp_lv_Bext\": \"Color channel B outside the lesion.\",\n",
    "    \"tbp_lv_C\": \"Chroma value inside the lesion.\",\n",
    "    \"tbp_lv_Cext\": \"Chroma value outside the lesion.\",\n",
    "    \"tbp_lv_H\": \"Hue value inside the lesion (LAB color space).\",\n",
    "    \"tbp_lv_Hext\": \"Hue value outside the lesion.\",\n",
    "    \"tbp_lv_L\": \"Luminance inside the lesion (LAB color space).\",\n",
    "    \"tbp_lv_Lext\": \"Luminance outside the lesion.\",\n",
    "    \"tbp_lv_areaMM2\": \"Area of the lesion in mm².\",\n",
    "    \"tbp_lv_area_perim_ratio\": \"Ratio of the lesion's perimeter to its area (border jaggedness).\",\n",
    "    \"tbp_lv_color_std_mean\": \"Mean color irregularity within the lesion.\",\n",
    "    \"tbp_lv_deltaA\": \"Average contrast in color channel A between inside and outside.\",\n",
    "    \"tbp_lv_deltaB\": \"Average contrast in color channel B between inside and outside.\",\n",
    "    \"tbp_lv_deltaL\": \"Average contrast in luminance between inside and outside.\",\n",
    "    \"tbp_lv_deltaLB\": \"Combined contrast between the lesion and surrounding skin.\",\n",
    "    \"tbp_lv_deltaLBnorm\": \"Normalized contrast (LAB color space).\",\n",
    "    \"tbp_lv_eccentricity\": \"Eccentricity of the lesion (how elongated it is).\",\n",
    "    \"tbp_lv_location\": \"Detailed anatomical location (e.g., Upper Arm).\",\n",
    "    \"tbp_lv_location_simple\": \"Simplified anatomical location (e.g., Arm).\",\n",
    "    \"tbp_lv_minorAxisMM\": \"Smallest diameter of the lesion in mm.\",\n",
    "    \"tbp_lv_nevi_confidence\": \"Confidence score (0-100) for the lesion being a nevus.\",\n",
    "    \"tbp_lv_norm_border\": \"Normalized border irregularity (0-10 scale).\",\n",
    "    \"tbp_lv_norm_color\": \"Normalized color variation (0-10 scale).\",\n",
    "    \"tbp_lv_perimeterMM\": \"Perimeter of the lesion in mm.\",\n",
    "    \"tbp_lv_radial_color_std_max\": \"Color asymmetry within the lesion, measured radially.\",\n",
    "    \"tbp_lv_stdL\": \"Std. deviation of luminance inside the lesion.\",\n",
    "    \"tbp_lv_stdLExt\": \"Std. deviation of luminance outside the lesion.\",\n",
    "    \"tbp_lv_symm_2axis\": \"Asymmetry about a second axis of symmetry.\",\n",
    "    \"tbp_lv_symm_2axis_angle\": \"Angle of that second axis of symmetry.\",\n",
    "    \"tbp_lv_x\": \"X-coordinate in the 3D TBP model.\",\n",
    "    \"tbp_lv_y\": \"Y-coordinate in the 3D TBP model.\",\n",
    "    \"tbp_lv_z\": \"Z-coordinate in the 3D TBP model.\",\n",
    "    \"attribution\": \"Image source or institution.\",\n",
    "    \"copyright_license\": \"License information.\",\n",
    "    \"lesion_id\": \"Unique ID for lesions of interest.\",\n",
    "    \"iddx_full\": \"Full diagnosis classification.\",\n",
    "    \"iddx_1\": \"First-level (broad) diagnosis.\",\n",
    "    \"iddx_2\": \"Second-level diagnosis.\",\n",
    "    \"iddx_3\": \"Third-level diagnosis.\",\n",
    "    \"iddx_4\": \"Fourth-level diagnosis.\",\n",
    "    \"iddx_5\": \"Fifth-level diagnosis.\",\n",
    "    \"mel_mitotic_index\": \"Mitotic index of invasive malignant melanomas.\",\n",
    "    \"mel_thick_mm\": \"Thickness of melanoma invasion in mm.\",\n",
    "    \"tbp_lv_dnn_lesion_confidence\": \"Lesion confidence score (0-100) from a DNN classifier.\",\n",
    "}\n",
    "\n",
    "METADATA_COL2NAME = {\n",
    "    \"isic_id\": \"Unique Case Identifier\",\n",
    "    \"target\": \"Binary Lesion Classification\",\n",
    "    \"patient_id\": \"Unique Patient Identifier\",\n",
    "    \"age_approx\": \"Approximate Age\",\n",
    "    \"sex\": \"Sex\",\n",
    "    \"anatom_site_general\": \"General Anatomical Location\",\n",
    "    \"clin_size_long_diam_mm\": \"Clinical Size (Longest Diameter in mm)\",\n",
    "    \"image_type\": \"Image Type\",\n",
    "    \"tbp_tile_type\": \"TBP Tile Type\",\n",
    "    \"tbp_lv_A\": \"Color Channel A (Inside)\",\n",
    "    \"tbp_lv_Aext\": \"Color Channel A (Outside)\",\n",
    "    \"tbp_lv_B\": \"Color Channel B (Inside)\",\n",
    "    \"tbp_lv_Bext\": \"Color Channel B (Outside)\",\n",
    "    \"tbp_lv_C\": \"Chroma (Inside)\",\n",
    "    \"tbp_lv_Cext\": \"Chroma (Outside)\",\n",
    "    \"tbp_lv_H\": \"Hue (Inside)\",\n",
    "    \"tbp_lv_Hext\": \"Hue (Outside)\",\n",
    "    \"tbp_lv_L\": \"Luminance (Inside)\",\n",
    "    \"tbp_lv_Lext\": \"Luminance (Outside)\",\n",
    "    \"tbp_lv_areaMM2\": \"Lesion Area (mm²)\",\n",
    "    \"tbp_lv_area_perim_ratio\": \"Area-to-Perimeter Ratio\",\n",
    "    \"tbp_lv_color_std_mean\": \"Mean Color Irregularity\",\n",
    "    \"tbp_lv_deltaA\": \"Delta A\",\n",
    "    \"tbp_lv_deltaB\": \"Delta B\",\n",
    "    \"tbp_lv_deltaL\": \"Delta L\",\n",
    "    \"tbp_lv_deltaLB\": \"Delta LB\",\n",
    "    \"tbp_lv_deltaLBnorm\": \"Normalized Delta LB\",\n",
    "    \"tbp_lv_eccentricity\": \"Eccentricity\",\n",
    "    \"tbp_lv_location\": \"Detailed Location\",\n",
    "    \"tbp_lv_location_simple\": \"Simplified Location\",\n",
    "    \"tbp_lv_minorAxisMM\": \"Smallest Diameter (mm)\",\n",
    "    \"tbp_lv_nevi_confidence\": \"Nevus Confidence Score\",\n",
    "    \"tbp_lv_norm_border\": \"Normalized Border Irregularity\",\n",
    "    \"tbp_lv_norm_color\": \"Normalized Color Variation\",\n",
    "    \"tbp_lv_perimeterMM\": \"Lesion Perimeter (mm)\",\n",
    "    \"tbp_lv_radial_color_std_max\": \"Radial Color Deviation\",\n",
    "    \"tbp_lv_stdL\": \"Std. Dev. Luminance (Inside)\",\n",
    "    \"tbp_lv_stdLExt\": \"Std. Dev. Luminance (Outside)\",\n",
    "    \"tbp_lv_symm_2axis\": \"Symmetry (Second Axis)\",\n",
    "    \"tbp_lv_symm_2axis_angle\": \"Symmetry Angle (Second Axis)\",\n",
    "    \"tbp_lv_x\": \"X-Coordinate\",\n",
    "    \"tbp_lv_y\": \"Y-Coordinate\",\n",
    "    \"tbp_lv_z\": \"Z-Coordinate\",\n",
    "    \"attribution\": \"Image Source\",\n",
    "    \"copyright_license\": \"Copyright\",\n",
    "    \"lesion_id\": \"Unique Lesion ID\",\n",
    "    \"iddx_full\": \"Full Diagnosis\",\n",
    "    \"iddx_1\": \"Diagnosis Level 1\",\n",
    "    \"iddx_2\": \"Diagnosis Level 2\",\n",
    "    \"iddx_3\": \"Diagnosis Level 3\",\n",
    "    \"iddx_4\": \"Diagnosis Level 4\",\n",
    "    \"iddx_5\": \"Diagnosis Level 5\",\n",
    "    \"mel_mitotic_index\": \"Mitotic Index (Melanoma)\",\n",
    "    \"mel_thick_mm\": \"Melanoma Thickness (mm)\",\n",
    "    \"tbp_lv_dnn_lesion_confidence\": \"Lesion DNN Confidence\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_92987/1963830541.py:261: DtypeWarning: Columns (51,52) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_df = pd.read_csv(TRAIN_METADATA_CSV)\n",
      "/tmp/ipykernel_92987/1963830541.py:266: DtypeWarning: Columns (52) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  valid_df = pd.read_csv(VALID_METADATA_CSV)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df shape: (320767, 55)\n",
      "valid_df shape: (80192, 55)\n",
      "test_df shape:  (100, 44)\n",
      "Train samples: 320767, Valid samples: 80192\n",
      "Target 0 count: 320499, Target 1 count: 268\n",
      "Adding 320231 augmented images to balance the dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_92987/1963830541.py:161: UserWarning: Argument 'limit' is not valid and will be ignored.\n",
      "  albumentations.RandomBrightnessContrast(limit=0.2, p=0.75),\n",
      "/home/rribes/.local/lib/python3.8/site-packages/pydantic/main.py:214: UserWarning: blur_limit and sigma_limit minimum value can not be both equal to 0. blur_limit minimum value changed to 3.\n",
      "  validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n",
      "[ WARN:0@2341.387] global loadsave.cpp:1329 imencode Unsupported depth image for selected encoder is fallbacked to CV_8U.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import h5py\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision import models\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import albumentations\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# If using GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Custom Dataset for HDF5\n",
    "# ---------------------------\n",
    "class ISIC_HDF5_Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset that loads images from an HDF5 file given a DataFrame of IDs.\n",
    "    Applies image transforms suitable for ResNet50.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        hdf5_path: str,\n",
    "        transform=None,\n",
    "        is_labelled: bool = True,\n",
    "        balanced=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df (pd.DataFrame): DataFrame containing 'isic_id' and optionally 'target'.\n",
    "            hdf5_path (str): Path to the HDF5 file containing images.\n",
    "            transform (callable): Optional transforms to be applied on a sample.\n",
    "            is_labelled (bool): Whether the dataset includes labels (for train/val).\n",
    "        \"\"\"\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.hdf5_path = hdf5_path\n",
    "        self.transform = transform\n",
    "        self.is_labelled = is_labelled\n",
    "        self.balanced = balanced\n",
    "\n",
    "        if self.balanced and self.is_labelled:\n",
    "            self._balance_dataset()\n",
    "\n",
    "    def _balance_dataset(self):  ## Added this function\n",
    "        \"\"\"Function to balance the dataset using SMOTE also converting non-numerical columns to numerical using LabelEncoder to be able to do the balancing.\"\"\"\n",
    "        # Store the original DataFrame\n",
    "        original_df = self.df.copy()\n",
    "        original_isic = self.df[\"isic_id\"].tolist()\n",
    "\n",
    "        # Augment images to balance the dataset\n",
    "        self._augment_images()\n",
    "\n",
    "        # Obtain a list of all new isic_id (added)\n",
    "        new_isic_ids = [\n",
    "            row[1][\"isic_id\"]\n",
    "            for row in self.df.iterrows()\n",
    "            if \"_aug_\" in row[1][\"isic_id\"]\n",
    "        ]\n",
    "        numerical_isic = [int(item.split(\"_\")[1]) for item in original_isic]\n",
    "\n",
    "        # Restore the original DataFrame\n",
    "        self.df = original_df\n",
    "\n",
    "        # Convert isiic_id to numerical\n",
    "        self.df.loc[:, \"isic_id\"] = numerical_isic\n",
    "        self.df[\"isic_id\"] = self.df[\"isic_id\"].astype(\"int64\")\n",
    "\n",
    "        non_numerical_data = self.df.select_dtypes(include=[\"object\"])\n",
    "\n",
    "        label_encoders = {}\n",
    "        for column in non_numerical_data.columns:\n",
    "            le = LabelEncoder()\n",
    "            self.df[column] = le.fit_transform(self.df[column].astype(str))\n",
    "            label_encoders[column] = le\n",
    "\n",
    "        # Fill NaN values in numerical columns with the mean\n",
    "        for column in self.df.select_dtypes(include=[\"float64\", \"int64\"]).columns:\n",
    "            self.df[column].fillna(self.df[column].mean(), inplace=True)\n",
    "\n",
    "        # Fill NaN values in categorical columns with the mode\n",
    "        for column in self.df.select_dtypes(include=[\"object\"]).columns:\n",
    "            self.df[column].fillna(self.df[column].mode()[0], inplace=True)\n",
    "\n",
    "        # Separate features and target\n",
    "\n",
    "        X = self.df.drop(columns=[\"target\"])  # Excluding target from dataset\n",
    "        y = self.df[\"target\"]\n",
    "\n",
    "        # Apply SMOTE\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_res, y_res = smote.fit_resample(X, y)\n",
    "\n",
    "        # Create a new DataFrame with balanced data\n",
    "        balanced_df = pd.DataFrame(X_res, columns=X.columns)\n",
    "        balanced_df[\"target\"] = y_res\n",
    "\n",
    "        # Update the DataFrame\n",
    "        self.df = balanced_df.reset_index(drop=True)\n",
    "\n",
    "        def format_isic_id(x):\n",
    "            x_str = str(x)  # Convert to string\n",
    "            if len(x_str) < 7:  # If length is less than 7, pad with leading zeros\n",
    "                x_str = x_str.zfill(7)  # zfill pads with leading zeros\n",
    "            return \"ISIC_\" + x_str\n",
    "\n",
    "        self.df[\"isic_id\"] = self.df[\"isic_id\"].apply(\n",
    "            format_isic_id\n",
    "        )  # Convert back to ISIC format\n",
    "        self.df[\"patient_id\"] = self.df[\"patient_id\"].apply(\n",
    "            lambda x: \"IP_\" + str(x)\n",
    "        )  # Convert back to patient ID format\n",
    "\n",
    "        # Change the isic_id of the augmented images to the new_isic_id to match with the images\n",
    "        for idx, row in self.df.iterrows():\n",
    "            if row[\"isic_id\"] not in original_isic and row[\"target\"] == 1:\n",
    "                new_isic_id = new_isic_ids.pop(0)\n",
    "                self.df.at[idx, \"isic_id\"] = new_isic_id\n",
    "\n",
    "        print(\"Dataset balanced using SMOTE.\\n New shape: \", self.df.shape)\n",
    "\n",
    "    def _augment_images(self):\n",
    "        \"\"\"Function to augment images to balance the dataset.\"\"\"\n",
    "        target_0_count = len(self.df[self.df[\"target\"] == 0])  # Count of target 0\n",
    "        target_1_count = len(self.df[self.df[\"target\"] == 1])  # Count of target 1\n",
    "        print(f\"Target 0 count: {target_0_count}, Target 1 count: {target_1_count}\")\n",
    "\n",
    "        # If target 0 count is greater than target 1 count, augment target 1 images\n",
    "        if target_0_count > target_1_count:\n",
    "            difference = target_0_count - target_1_count\n",
    "            target_1_df = self.df[self.df[\"target\"] == 1]\n",
    "            augmented_images = []\n",
    "            print(f\"Adding {difference} augmented images to balance the dataset.\")\n",
    "# Augment images until the classes are equal\n",
    "            while len(augmented_images) < difference:\n",
    "                for idx, row in target_1_df.iterrows():\n",
    "                    if len(augmented_images) >= difference:\n",
    "                        break\n",
    "                    isic_id = row[\"isic_id\"]  # Get the ISIC ID\n",
    "                    image_rgb = self._load_image_from_hdf5(isic_id)  # Load the image from HDF5\n",
    "                    import torchvision.transforms.functional as F_v\n",
    "                    image_pil = F_v.to_pil_image(image_rgb)  # Convert to PIL Image for transform\n",
    "                    # Apply random augmentations to the images\n",
    "                    augmentation_transforms = albumentations.Compose([\n",
    "                        albumentations.Transpose(p=0.5),\n",
    "                        albumentations.VerticalFlip(p=0.5),\n",
    "                        albumentations.HorizontalFlip(p=0.5),\n",
    "                        albumentations.RandomBrightnessContrast(limit=0.2, p=0.75),\n",
    "                        albumentations.OneOf([\n",
    "                            albumentations.MotionBlur(blur_limit=5),\n",
    "                            albumentations.MedianBlur(blur_limit=5),\n",
    "                            albumentations.GaussianBlur(blur_limit=5),\n",
    "                            albumentations.GaussNoise(var_limit=(5.0, 30.0)),\n",
    "                        ], p=0.7),\n",
    "                        albumentations.OneOf([\n",
    "                            albumentations.OpticalDistortion(distort_limit=1.0),\n",
    "                            albumentations.GridDistortion(num_steps=5, distort_limit=1.0),\n",
    "                            albumentations.ElasticTransform(alpha=3),\n",
    "                        ], p=0.7),\n",
    "                        albumentations.CLAHE(clip_limit=4.0, p=0.7),\n",
    "                        albumentations.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=20, val_shift_limit=10, p=0.5),\n",
    "                        albumentations.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, border_mode=0, p=0.85),\n",
    "                        albumentations.Normalize()\n",
    "                    ])\n",
    "                    augmented = augmentation_transforms(image=np.array(image_pil))\n",
    "                    # Extract the transformed image from the output dictionary\n",
    "                    augmented_image_np = augmented[\"image\"]\n",
    "                    _, encoded_image = cv2.imencode('.jpg', cv2.cvtColor(augmented_image_np, cv2.COLOR_RGB2BGR))\n",
    "                    \n",
    "                    new_isic_id = f\"{isic_id}_aug_{len(augmented_images)}\"  # Create a new ISIC ID\n",
    "                    augmented_images.append((new_isic_id, encoded_image, row))\n",
    "                ## break ##\n",
    "\n",
    "            # Add augmented images to the existing HDF5 file and DataFrame\n",
    "            with h5py.File(self.hdf5_path, \"a\") as hf:\n",
    "                for new_isic_id, encoded_image, row in augmented_images:\n",
    "                    hf.create_dataset(new_isic_id, data=encoded_image)\n",
    "                    new_row = row.copy()\n",
    "                    new_row[\"isic_id\"] = new_isic_id\n",
    "                    self.df = pd.concat(\n",
    "                        [self.df, pd.DataFrame([new_row])], ignore_index=True\n",
    "                    )\n",
    "                    ## break ##\n",
    "                hf.close()\n",
    "\n",
    "            print(\n",
    "                f\"Added {len(augmented_images)} augmented images to balance the dataset.\"\n",
    "            )\n",
    "            print(f\"New dataset shape: {self.df.shape}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        isic_id = row[\"isic_id\"]\n",
    "\n",
    "        # Load image from HDF5\n",
    "        image_rgb = self._load_image_from_hdf5(isic_id)\n",
    "\n",
    "        # Apply transforms (PIL-style transforms require converting np array to PIL, or we can do tensor transforms)\n",
    "        if self.transform is not None:\n",
    "            # Convert NumPy array (H x W x C) to a PIL Image\n",
    "            import torchvision.transforms.functional as F_v\n",
    "\n",
    "            image_pil = F_v.to_pil_image(image_rgb)\n",
    "            image = self.transform(image_pil)\n",
    "        else:\n",
    "            # By default, convert it to a tensor (C x H x W)\n",
    "            image = torch.from_numpy(image_rgb).permute(2, 0, 1).float()\n",
    "\n",
    "        if self.is_labelled:\n",
    "            label = row[\"target\"]\n",
    "            label = torch.tensor(label).float()\n",
    "            return image, label, isic_id\n",
    "        else:\n",
    "            return image, isic_id\n",
    "\n",
    "    def _load_image_from_hdf5(self, isic_id: str):\n",
    "        \"\"\"\n",
    "        Loads and decodes an image from HDF5 by isic_id.\n",
    "        Returns a NumPy array in RGB format (H x W x 3).\n",
    "        \"\"\"\n",
    "        with h5py.File(self.hdf5_path, \"r\") as hf:\n",
    "            encoded_bytes = hf[isic_id][()]  # uint8 array\n",
    "\n",
    "        # Decode the image bytes with OpenCV (returns BGR)\n",
    "        image_bgr = cv2.imdecode(encoded_bytes, cv2.IMREAD_COLOR)\n",
    "        # Convert to RGB\n",
    "        image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "        return image_rgb\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 2. DataFrames and Basic Preprocessing / Transforms\n",
    "# ----------------------------------------------------\n",
    "# -----------------------------\n",
    "# 3. Data Setup (Train/Valid/Test)\n",
    "# -----------------------------\n",
    "TRAIN_METADATA_CSV = \"data/new-train-metadata.csv\"\n",
    "TEST_METADATA_CSV = \"data/students-test-metadata.csv\"\n",
    "VALID_METADATA_CSV = \"data/new-valid-metadata.csv\"\n",
    "\n",
    "TRAIN_HDF5 = \"data/train-image.hdf5\"\n",
    "TEST_HDF5 = \"data/test-image.hdf5\"\n",
    "VALID_HDF5 = \"data/valid-image.hdf5\"\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_METADATA_CSV)\n",
    "train_df = train_df.sample(frac=0.02, random_state=42).reset_index(\n",
    "    drop=True\n",
    ")  # Sample 2% of the data\n",
    "test_df = pd.read_csv(TEST_METADATA_CSV)\n",
    "valid_df = pd.read_csv(VALID_METADATA_CSV)\n",
    "\n",
    "print(f\"train_df shape: {train_df.shape}\")\n",
    "print(f\"valid_df shape: {valid_df.shape}\")\n",
    "print(f\"test_df shape:  {test_df.shape}\")\n",
    "\n",
    "\n",
    "##\n",
    "# Example: split train_df into 80% train / 20% valid\n",
    "# train_size = int(0.8 * len(train_df))\n",
    "# valid_size = len(train_df) - train_size\n",
    "# train_subset, valid_subset = random_split(\n",
    "#     train_df,\n",
    "#     [train_size, valid_size],\n",
    "#     generator=torch.Generator().manual_seed(42)\n",
    "# )\n",
    "\n",
    "# train_df_sub = train_df.iloc[train_subset.indices].reset_index(drop=True)\n",
    "# valid_df_sub = train_df.iloc[valid_subset.indices].reset_index(drop=True)\n",
    "\n",
    "# print(f\"Train samples: {len(train_df_sub)}, Valid samples: {len(valid_df_sub)}\")\n",
    "##\n",
    "\n",
    "print(f\"Train samples: {len(train_df)}, Valid samples: {len(valid_df)}\")\n",
    "\n",
    "# Basic transforms for ResNet\n",
    "resnet_transforms = T.Compose(\n",
    "    [\n",
    "        T.Resize((224, 224)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create Datasets\n",
    "train_dataset = ISIC_HDF5_Dataset(\n",
    "    df=train_df,  # Change to train_df if using the original train_df\n",
    "    hdf5_path=TRAIN_HDF5,\n",
    "    transform=resnet_transforms,\n",
    "    is_labelled=True\n",
    "    #balanced=True,  # Balanced dataset for training\n",
    ")\n",
    "\n",
    "valid_dataset = ISIC_HDF5_Dataset(\n",
    "    df=valid_df,  # Change to valid_df if using the original valid_df\n",
    "    hdf5_path=VALID_HDF5,  # Change to VALID_HDF5 if using the original valid_df\n",
    "    transform=resnet_transforms,\n",
    "    is_labelled=True,\n",
    ")\n",
    "\n",
    "test_dataset = ISIC_HDF5_Dataset(\n",
    "    df=test_df, hdf5_path=TEST_HDF5, transform=resnet_transforms, is_labelled=False\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Created train/valid/test datasets.\")\n",
    "\n",
    "\n",
    "def store_validation_set(valid_dataset, valid_df_sub):\n",
    "    # Store the images for valid_dataset in a file named valid-image.hdf5\n",
    "    valid_hdf5_path = \"data/valid-image.hdf5\"\n",
    "    with h5py.File(valid_hdf5_path, \"w\") as hf:\n",
    "        for idx in range(len(valid_dataset)):\n",
    "            image, label, isic_id = valid_dataset[idx]\n",
    "            # Convert tensor to numpy array and encode as JPEG\n",
    "            image_np = image.permute(1, 2, 0).numpy().astype(np.uint8)\n",
    "            _, encoded_image = cv2.imencode(\n",
    "                \".jpg\", cv2.cvtColor(image_np, cv2.COLOR_RGB2BGR)\n",
    "            )\n",
    "            hf.create_dataset(isic_id, data=encoded_image)\n",
    "\n",
    "    # Store the dataframe for valid_dataset as new-valid-metadata.csv\n",
    "    valid_df_sub.to_csv(\"data/new-valid-metadata.csv\", index=False)\n",
    "    print(\"Validation sets stored at data/.\")\n",
    "\n",
    "\n",
    "def store_training_set(train_dataset, train_df_sub):\n",
    "    # Store the images for train_dataset in a file named train-image.hdf5\n",
    "    train_hdf5_path = \"data/train-image.hdf5\"\n",
    "    with h5py.File(train_hdf5_path, \"w\") as hf:\n",
    "        for idx in range(len(train_dataset)):\n",
    "            image, label, isic_id = train_dataset[idx]\n",
    "            # Convert tensor to numpy array and encode as JPEG\n",
    "            image_np = image.permute(1, 2, 0).numpy().astype(np.uint8)\n",
    "            _, encoded_image = cv2.imencode(\n",
    "                \".jpg\", cv2.cvtColor(image_np, cv2.COLOR_RGB2BGR)\n",
    "            )\n",
    "            hf.create_dataset(isic_id, data=encoded_image)\n",
    "\n",
    "    # Store the dataframe for train_dataset as new-train-metadata.csv\n",
    "    train_df_sub.to_csv(\"data/new-train-metadata.csv\", index=False)\n",
    "    print(\"Training sets stored at data/.\")\n",
    "\n",
    "\n",
    "# store_validation_set(valid_dataset,valid_df_sub)\n",
    "# store_training_set(train_dataset,train_df_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader: 125 batches (total = 1000 samples / batch_size = 8)\n",
      "Valid loader: 10024 batches\n",
      "Test loader:  13 batches\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 4. RandomSampler for 1000 Samples per Epoch\n",
    "# -----------------------------\n",
    "# Instead of weighting for class imbalance, we simply draw 1000 random samples each epoch.\n",
    "# Students can discover the imbalance issue themselves!\n",
    "\n",
    "from torch.utils.data import RandomSampler\n",
    "\n",
    "# A RandomSampler with replacement=True can pick num_samples=1000 each epoch\n",
    "sampler = RandomSampler(data_source=train_dataset, replacement=True, num_samples=1000)\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sampler=sampler,  # No shuffle needed, RandomSampler handles randomness\n",
    "    num_workers=2,\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Train loader: {len(train_loader)} batches (total = 1000 samples / batch_size = {BATCH_SIZE})\"\n",
    ")\n",
    "print(f\"Valid loader: {len(valid_loader)} batches\")\n",
    "print(f\"Test loader:  {len(test_loader)} batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rribes/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/rribes/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 5. Load & Modify ResNet50 for Binary Classification\n",
    "# -----------------------------\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "\n",
    "model = models.resnet50(pretrained=True)\n",
    "\n",
    "# Replace final layer for binary classification\n",
    "model.fc = nn.Linear(in_features=2048, out_features=1)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DINO v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, AutoModel\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained('/home/rribes/.cache/kagglehub/models/metaresearch/dinov2/pyTorch/base/1') #hardcoded\n",
    "model = AutoModel.from_pretrained('/home/rribes/.cache/kagglehub/models/metaresearch/dinov2/pyTorch/base/1') #hardcoded\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "last_hidden_states = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/10024 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mEl kernel se bloqueó al ejecutar código en la celda actual o en una celda anterior. \n",
      "\u001b[1;31mRevise el código de las celdas para identificar una posible causa del error. \n",
      "\u001b[1;31mHaga clic <a href='https://aka.ms/vscodeJupyterKernelCrash'>aquí</a> para obtener más información. \n",
      "\u001b[1;31mVea Jupyter <a href='command:jupyter.viewOutput'>log</a> para obtener más detalles."
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 6. Training Loop (5 Epochs, Only 1000 Samples/Epoch)\n",
    "# -----------------------------\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "model.fc = nn.Linear(in_features=2048, out_features=1)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Unfreeze all layers for fine-tuning (if desired)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "EPOCHS = 5\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, labels, _ in tqdm(valid_loader, desc=f\"Epoch {epoch}\", leave=False):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(images).view(-1)  # [batch_size]\n",
    "\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch}/{EPOCHS} | Train Loss: {avg_train_loss:.4f}))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.8)\n",
      "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/models/xukongji/resnet18/pyTorch/resnet18/1/download/resnet18_baseline_112/best_val_model_fold-0.pt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0.00/42.7M [00:00<?, ?B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/models/xukongji/resnet18/pyTorch/resnet18/1/download/resnet18_baseline_112/best_val_model_fold-1.pt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/models/xukongji/resnet18/pyTorch/resnet18/1/download/resnet18_baseline_112/best_val_model_fold-2.pt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/models/xukongji/resnet18/pyTorch/resnet18/1/download/resnet18_baseline_112/best_val_model_fold-4.pt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/models/xukongji/resnet18/pyTorch/resnet18/1/download/resnet18_baseline_112/best_val_model_fold-3.pt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████| 42.7M/42.7M [00:02<00:00, 15.2MB/s]\n",
      "Downloading 5 files:  20%|██        | 1/5 [00:03<00:14,  3.62s/it]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 42.7M/42.7M [00:03<00:00, 11.6MB/s]\n",
      "Downloading 5 files:  40%|████      | 2/5 [00:04<00:06,  2.02s/it]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 42.7M/42.7M [00:05<00:00, 7.47MB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 42.7M/42.7M [00:06<00:00, 7.06MB/s]\n",
      "Downloading 5 files:  60%|██████    | 3/5 [00:07<00:04,  2.43s/it]\n",
      "\n",
      "\n",
      "100%|██████████| 42.7M/42.7M [00:06<00:00, 7.04MB/s]\n",
      "Downloading 5 files: 100%|██████████| 5/5 [00:07<00:00,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to model files: /home/rribes/.cache/kagglehub/models/xukongji/resnet18/pyTorch/resnet18/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "#path = kagglehub.model_download(\"xukongji/resnet18/pyTorch/resnet18\")\n",
    "\n",
    "#print(\"Path to model files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 | Loss: 0.0602\n",
      "Epoch 2/5 | Loss: 0.0218\n",
      "Epoch 3/5 | Loss: 0.0106\n",
      "Epoch 4/5 | Loss: 0.0101\n",
      "Epoch 5/5 | Loss: 0.0015\n"
     ]
    }
   ],
   "source": [
    "# Load the pretrained model (assumes it was saved as a complete model)\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from torchvision import models\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Retrieve all .pt files from the specified folder and choose one (the latest in this example)\n",
    "path = \"/home/rribes/.cache/kagglehub/models/xukongji/resnet18/pyTorch/resnet18/1/resnet18_baseline_112/\" # hardcoded\n",
    "pt_files = glob.glob(os.path.join(path, '*.pt'))\n",
    "pt_files.sort()  # sort for consistency\n",
    "checkpoint_path = pt_files[-1]\n",
    "\n",
    "model_ft = torch.load(checkpoint_path, map_location=torch.device(\"cpu\"))\n",
    "\n",
    "# Modify the final layer for binary classification\n",
    "model_ft_temp = models.resnet18()\n",
    "model_ft_temp.load_state_dict(model_ft, strict=False)\n",
    "num_features = model_ft_temp.fc.in_features\n",
    "model_ft = model_ft_temp\n",
    "model_ft.fc = nn.Linear(num_features, 1)\n",
    "\n",
    "model_ft = model_ft.to(device=device)\n",
    "\n",
    "criterion_ft = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer_ft = optim.Adam(model_ft.parameters(), lr=1e-4)\n",
    "\n",
    "EPOCHS = 5  # or any number you prefer\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model_ft.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, labels, _ in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer_ft.zero_grad()\n",
    "        logits = model_ft(images).view(-1)\n",
    "        loss = criterion_ft(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer_ft.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch}/{EPOCHS} | Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference on Test: 100%|██████████| 13/13 [00:03<00:00,  3.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved submission with 100 rows to submission.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isic_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ISIC_0082829</td>\n",
       "      <td>0.001589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ISIC_0114227</td>\n",
       "      <td>0.000549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ISIC_0157465</td>\n",
       "      <td>0.001845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ISIC_0197356</td>\n",
       "      <td>0.000336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ISIC_0275647</td>\n",
       "      <td>0.000408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ISIC_0332355</td>\n",
       "      <td>0.000520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ISIC_0528190</td>\n",
       "      <td>0.000398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ISIC_0576478</td>\n",
       "      <td>0.002629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ISIC_0719839</td>\n",
       "      <td>0.000418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ISIC_0968965</td>\n",
       "      <td>0.000521</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        isic_id    target\n",
       "0  ISIC_0082829  0.001589\n",
       "1  ISIC_0114227  0.000549\n",
       "2  ISIC_0157465  0.001845\n",
       "3  ISIC_0197356  0.000336\n",
       "4  ISIC_0275647  0.000408\n",
       "5  ISIC_0332355  0.000520\n",
       "6  ISIC_0528190  0.000398\n",
       "7  ISIC_0576478  0.002629\n",
       "8  ISIC_0719839  0.000418\n",
       "9  ISIC_0968965  0.000521"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 7. Inference on Test Set & Submission\n",
    "# -----------------------------\n",
    "model = model_ft\n",
    "model.eval()\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, isic_ids in tqdm(test_loader, desc=\"Inference on Test\"):\n",
    "        images = images.to(device)\n",
    "        logits = model(images).view(-1)  # shape [batch_size]\n",
    "        probs = torch.sigmoid(logits)  # shape [batch_size], in [0,1]\n",
    "\n",
    "        probs = probs.cpu().numpy()\n",
    "\n",
    "        for isic_id, p in zip(isic_ids, probs):\n",
    "            predictions.append({\"isic_id\": isic_id, \"target\": float(p)})\n",
    "\n",
    "submission_df = pd.DataFrame(predictions)\n",
    "submission_df = submission_df.sort_values(by=\"isic_id\").reset_index(drop=True)\n",
    "\n",
    "submission_file = \"submission.csv\"\n",
    "submission_df.to_csv(submission_file, index=False)\n",
    "\n",
    "print(f\"Saved submission with {len(submission_df)} rows to {submission_file}\")\n",
    "display(submission_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
